{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/shnakazawa/object-detection-with-pytorch-and-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\git\\hwars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd c:\\git\\hwars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.0+cu118\n",
      "Albumentations version 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Image augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "print(f'PyTorch version {torch.__version__}')\n",
    "print(f'Albumentations version {A.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EDA = True\n",
    "RUN_TRAINING = True\n",
    "TRAIN_ALL = False # If true, train with all data and output a single model. If False, run cross-validation and output multiple models.\n",
    "FOLD_NUM = 5 # For cross-validation\n",
    "EPOCHS = 3\n",
    "RUN_INFERENCE = False\n",
    "\n",
    "# Directory setting\n",
    "DETR_DIR = 'detr'\n",
    "DATA_DIR = 'data/kaggle/input/global-wheat-detection/'\n",
    "MODEL_DIR = 'data/kaggle/working/'\n",
    "IMG_SAVE_DIR = 'data/kaggle/working/'\n",
    "\n",
    "# PyTorch variables\n",
    "SEED = 42\n",
    "NUM_CLASSES = 2 # Wheat + background = 2 classes\n",
    "# NUM_QUERIES = 100 # How many bboxes shall the model output per image. In this notebook, it is defined based on the num of bboxes in training data\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "\n",
    "NULL_CLASS_COEF = 0.5\n",
    "GAMMA = 0.1\n",
    "THRESHOLD = 0.3 # Threshold for bbox prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DETR_DIR) == False:\n",
    "    !git clone https://github.com/facebookresearch/detr.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('detr')\n",
    "\n",
    "from detr.models.matcher import HungarianMatcher\n",
    "from detr.models.detr import SetCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set seed\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "def show_gpu_memory(device):\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(device) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Cached GPU memory: {torch.cuda.memory_cached(device) / 1024 / 1024:.2f} MB\")    \n",
    "\n",
    "    \n",
    "def load_img(path):\n",
    "    img_bgr = cv2.imread(path)\n",
    "    img_rgb = img_bgr[:, :, ::-1]\n",
    "    return img_rgb\n",
    "\n",
    "\n",
    "def group_bboxes(df):\n",
    "    df_ = df.copy()\n",
    "    df_['bbox_count'] = 1\n",
    "    df_ = df_.groupby(['image_id', 'source']).count().reset_index()\n",
    "    return_df = df_[['image_id', 'source', 'bbox_count']]\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def create_gallery(array, ncols=3):\n",
    "    \"\"\"Display multiple images in a gallery style.\n",
    "    Source: https://www.amazon.co.jp/Data-Analysis-Machine-Learning-Kaggle-ebook/dp/B09F3STL34/\n",
    "    \n",
    "    Args:\n",
    "        array (numpy.ndarray): array of images.\n",
    "        ncols (int, optional): Num of columns. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: One concatenated image.\n",
    "    \"\"\"    \n",
    "    nindex, height, width, intensity = array.shape\n",
    "    nrows = nindex//ncols\n",
    "    assert nindex == nrows * ncols\n",
    "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
    "        .swapaxes(1,2)\n",
    "        .reshape(height*nrows, width*ncols, intensity))\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalize_bbox(bboxes, rows, cols):\n",
    "    \"\"\"Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates\n",
    "    by image height.\n",
    "    Source: https://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/_modules/albumentations/augmentations/bbox_utils.html\n",
    "    with minor modification\n",
    "    \"\"\"\n",
    "    normalized_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        normalized_bbox = (x_min / cols, y_min / rows, x_max / cols, y_max / rows)\n",
    "        normalized_bboxes.append(normalized_bbox)\n",
    "    return normalized_bboxes\n",
    "\n",
    "\n",
    "def denormalize_bbox(bboxes, rows, cols):\n",
    "    \"\"\"Denormalize coordinates of a bounding box. Multiply x-coordinates by image width and y-coordinates\n",
    "    by image height. This is an inverse operation for :func:`~albumentations.augmentations.bbox.normalize_bbox`.\n",
    "    Source: https://vfdev-5-albumentations.readthedocs.io/en/docs_pytorch_fix/_modules/albumentations/augmentations/bbox_utils.html\n",
    "    with minor modification\n",
    "    \"\"\"\n",
    "    denormalized_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        denormalized_bbox = (x_min * cols, y_min * rows, x_max * cols, y_max * rows)\n",
    "        denormalized_bboxes.append(denormalized_bbox)\n",
    "    return denormalized_bboxes\n",
    "\n",
    "\n",
    "def show_validation_score(train_loss_list, valid_loss_list, save=False, save_dir=IMG_SAVE_DIR, save_name='objectdetection_validation_score.png'):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in range(FOLD_NUM):\n",
    "        train_loss = train_loss_list[i]\n",
    "        valid_loss = valid_loss_list[i]\n",
    "        \n",
    "        ax = fig.add_subplot(math.ceil(np.sqrt(FOLD_NUM)), math.ceil(np.sqrt(FOLD_NUM)), i+1, title=f'Fold {i+1}')\n",
    "        ax.plot(range(EPOCHS), train_loss, c='orange', label='train')\n",
    "        ax.plot(range(EPOCHS), valid_loss, c='blue', label='valid')\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(save_dir+save_name)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kaggle/input/global-wheat-detection/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(DATA_DIR \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    620\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1879\u001b[0m     f,\n\u001b[0;32m   1880\u001b[0m     mode,\n\u001b[0;32m   1881\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1882\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1883\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1884\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1885\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1886\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1887\u001b[0m )\n\u001b[0;32m   1888\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/kaggle/input/global-wheat-detection/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwars-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
