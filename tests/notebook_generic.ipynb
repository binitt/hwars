{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\git\\hwars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from_file = r'data/buttons/hwars.json'\n",
    "with open(from_file, \"r\") as f:\n",
    "    from_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_via_settings', '_via_img_metadata', '_via_attributes', '_via_data_format_version', '_via_image_id_list'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_obj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'ss-1-find.png',\n",
       " 'size': 807336,\n",
       " 'regions': [{'shape_attributes': {'name': 'rect',\n",
       "    'x': 1838,\n",
       "    'y': 313,\n",
       "    'width': 90,\n",
       "    'height': 84},\n",
       "   'region_attributes': {'text': 'X'}},\n",
       "  {'shape_attributes': {'name': 'rect',\n",
       "    'x': 814,\n",
       "    'y': 1051,\n",
       "    'width': 229,\n",
       "    'height': 79},\n",
       "   'region_attributes': {'text': 'OK'}},\n",
       "  {'shape_attributes': {'name': 'rect',\n",
       "    'x': 1629,\n",
       "    'y': 746,\n",
       "    'width': 210,\n",
       "    'height': 76},\n",
       "   'region_attributes': {'text': 'Find'}},\n",
       "  {'shape_attributes': {'name': 'rect',\n",
       "    'x': 1631,\n",
       "    'y': 855,\n",
       "    'width': 210,\n",
       "    'height': 76},\n",
       "   'region_attributes': {'text': 'Find'}},\n",
       "  {'shape_attributes': {'name': 'rect',\n",
       "    'x': 1629,\n",
       "    'y': 962,\n",
       "    'width': 210,\n",
       "    'height': 76},\n",
       "   'region_attributes': {'text': 'Find'}},\n",
       "  {'shape_attributes': {'name': 'rect',\n",
       "    'x': 1626,\n",
       "    'y': 643,\n",
       "    'width': 210,\n",
       "    'height': 76},\n",
       "   'region_attributes': {'text': 'Find'}}],\n",
       " 'file_attributes': {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(from_obj['_via_img_metadata'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_entry = next(iter(from_obj['_via_img_metadata'].values()))\n",
    "to_entry = {\n",
    "    \"file_name\": from_entry['filename'],\n",
    "    \"objects\": {\n",
    "        \"bbox\": []\n",
    "    },\n",
    "    \"categories\": [0]\n",
    "}\n",
    "\n",
    "for r in from_entry['regions']:\n",
    "    sa = r['shape_attributes']\n",
    "    if sa['name'] != 'rect':\n",
    "        raise Exception(f'Only rect is supported but got {sa[\"name\"]}')\n",
    "    to_entry[\"objects\"][\"bbox\"].append([sa['x'], sa['y'], sa['width'], sa['height']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'ss-1-find.png',\n",
       " 'objects': {'bbox': [[1838, 313, 90, 84],\n",
       "   [814, 1051, 229, 79],\n",
       "   [1629, 746, 210, 76],\n",
       "   [1631, 855, 210, 76],\n",
       "   [1629, 962, 210, 76],\n",
       "   [1626, 643, 210, 76]]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7370f7862f4a348fe6922bdfb9be42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653408d73c0a4db09433cc4496140b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f692ed945f46128f6637a77fd62ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726f30873a5246e5a9ca10e330f7d701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\hwars\\hwars-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\binit\\.cache\\huggingface\\hub\\datasets--binitt--hwars-buttons. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/binitt/hwars-buttons/commit/fcdd74f246d26cbf58fd452bd6f100b283b6e845', commit_message='Upload dataset', commit_description='', oid='fcdd74f246d26cbf58fd452bd6f100b283b6e845', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"data/buttons\", split=\"train\")\n",
    "dataset.push_to_hub(\"binitt/hwars-buttons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "    num_rows: 7\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2560x1440>,\n",
       " 'image_id': 6,\n",
       " 'width': 2560,\n",
       " 'height': 1440,\n",
       " 'objects': {'id': [18, 19, 20],\n",
       "  'area': [18620, 34692, 31590],\n",
       "  'bbox': [[1569, 490, 196, 95], [910, 1149, 354, 98], [1302, 1155, 351, 90]],\n",
       "  'category': [0]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"binitt/hwars-buttons\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2560x1440>,\n",
       " 'image_id': 6,\n",
       " 'width': 2560,\n",
       " 'height': 1440,\n",
       " 'objects': {'id': [18, 19, 20],\n",
       "  'area': [18620, 34692, 31590],\n",
       "  'bbox': [[1569, 490, 196, 95], [910, 1149, 354, 98], [1302, 1155, 351, 90]],\n",
       "  'category': [0]}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import logging\n",
    "from transformers import AutoImageProcessor\n",
    "import albumentations\n",
    "import numpy as np\n",
    "from transformers import AutoModelForObjectDetection\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "categories = [\"button\"]\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    print(f\"Aug with {len(examples)}\")\n",
    "    transform = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(480, 480),\n",
    "            albumentations.HorizontalFlip(p=1.0),\n",
    "            albumentations.RandomBrightnessContrast(p=1.0),\n",
    "        ],\n",
    "        bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    "    )\n",
    "\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2560x1440>,\n",
       " 'image_id': 6,\n",
       " 'width': 2560,\n",
       " 'height': 1440,\n",
       " 'objects': {'id': [18, 19, 20],\n",
       "  'area': [18620, 34692, 31590],\n",
       "  'bbox': [[1569, 490, 196, 95], [910, 1149, 354, 98], [1302, 1155, 351, 90]],\n",
       "  'category': [0]}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform_aug_ann([dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]])\n",
      "Cell \u001b[1;32mIn[43], line 26\u001b[0m, in \u001b[0;36mtransform_aug_ann\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_aug_ann\u001b[39m(examples):\n\u001b[0;32m     17\u001b[0m     transform \u001b[39m=\u001b[39m albumentations\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     18\u001b[0m         [\n\u001b[0;32m     19\u001b[0m             albumentations\u001b[39m.\u001b[39mResize(\u001b[39m480\u001b[39m, \u001b[39m480\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         bbox_params\u001b[39m=\u001b[39malbumentations\u001b[39m.\u001b[39mBboxParams(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoco\u001b[39m\u001b[39m\"\u001b[39m, label_fields\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     24\u001b[0m     )\n\u001b[1;32m---> 26\u001b[0m     image_ids \u001b[39m=\u001b[39m examples[\u001b[39m\"\u001b[39;49m\u001b[39mimage_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     27\u001b[0m     images, bboxes, area, categories \u001b[39m=\u001b[39m [], [], [], []\n\u001b[0;32m     28\u001b[0m     \u001b[39mfor\u001b[39;00m image, objects \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(examples[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], examples[\u001b[39m\"\u001b[39m\u001b[39mobjects\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "transform_aug_ann([dataset[\"train\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset = dataset[\"train\"].with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# format annotations the same as for training, no need for data augmentation\n",
    "def val_formatted_anns(image_id, objects):\n",
    "    annotations = []\n",
    "    for i in range(0, len(objects[\"id\"])):\n",
    "        new_ann = {\n",
    "            \"id\": objects[\"id\"][i],\n",
    "            \"category_id\": objects[\"category\"][i],\n",
    "            \"iscrowd\": 0,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": objects[\"area\"][i],\n",
    "            \"bbox\": objects[\"bbox\"][i],\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Save images and annotations into the files torchvision.datasets.CocoDetection expects\n",
    "def save_annotation_file_images(dataset):\n",
    "    output_json = {}\n",
    "    path_output = f\"data/output-buttons/\"\n",
    "\n",
    "    if not os.path.exists(path_output):\n",
    "        os.makedirs(path_output)\n",
    "\n",
    "    path_anno = os.path.join(path_output, \"buttons_ann.json\")\n",
    "    categories_json = [{\"supercategory\": \"none\", \"id\": id, \"name\": id2label[id]} for id in id2label]\n",
    "    output_json[\"images\"] = []\n",
    "    output_json[\"annotations\"] = []\n",
    "    for example in dataset:\n",
    "        ann = val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n",
    "        output_json[\"images\"].append(\n",
    "            {\n",
    "                \"id\": example[\"image_id\"],\n",
    "                \"width\": example[\"image\"].width,\n",
    "                \"height\": example[\"image\"].height,\n",
    "                \"file_name\": f\"{example['image_id']}.png\",\n",
    "            }\n",
    "        )\n",
    "        output_json[\"annotations\"].extend(ann)\n",
    "    output_json[\"categories\"] = categories_json\n",
    "\n",
    "    with open(path_anno, \"w\") as file:\n",
    "        json.dump(output_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for im, img_id in zip(dataset[\"image\"], dataset[\"image_id\"]):\n",
    "        path_img = os.path.join(path_output, f\"{img_id}.png\")\n",
    "        im.save(path_img)\n",
    "\n",
    "    return path_output, path_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: pixel_values, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: target}\n\u001b[0;32m     25\u001b[0m im_processor \u001b[39m=\u001b[39m AutoImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdevonho/detr-resnet-50_finetuned_cppe5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m path_output, path_anno \u001b[39m=\u001b[39m save_annotation_file_images(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     28\u001b[0m test_ds_coco_format \u001b[39m=\u001b[39m CocoDetection(path_output, im_processor, path_anno)\n",
      "Cell \u001b[1;32mIn[63], line 34\u001b[0m, in \u001b[0;36msave_annotation_file_images\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     32\u001b[0m output_json[\u001b[39m\"\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m---> 34\u001b[0m     ann \u001b[39m=\u001b[39m val_formatted_anns(example[\u001b[39m\"\u001b[39;49m\u001b[39mimage_id\u001b[39;49m\u001b[39m\"\u001b[39;49m], example[\u001b[39m\"\u001b[39;49m\u001b[39mobjects\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     35\u001b[0m     output_json[\u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     36\u001b[0m         {\n\u001b[0;32m     37\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: example[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m         }\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     output_json[\u001b[39m\"\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(ann)\n",
      "Cell \u001b[1;32mIn[63], line 10\u001b[0m, in \u001b[0;36mval_formatted_anns\u001b[1;34m(image_id, objects)\u001b[0m\n\u001b[0;32m      6\u001b[0m annotations \u001b[39m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(objects[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[0;32m      8\u001b[0m     new_ann \u001b[39m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: objects[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m][i],\n\u001b[1;32m---> 10\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcategory_id\u001b[39m\u001b[39m\"\u001b[39m: objects[\u001b[39m\"\u001b[39;49m\u001b[39mcategory\u001b[39;49m\u001b[39m\"\u001b[39;49m][i],\n\u001b[0;32m     11\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39miscrowd\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m: image_id,\n\u001b[0;32m     13\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marea\u001b[39m\u001b[39m\"\u001b[39m: objects[\u001b[39m\"\u001b[39m\u001b[39marea\u001b[39m\u001b[39m\"\u001b[39m][i],\n\u001b[0;32m     14\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m\"\u001b[39m: objects[\u001b[39m\"\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m\"\u001b[39m][i],\n\u001b[0;32m     15\u001b[0m     }\n\u001b[0;32m     16\u001b[0m     annotations\u001b[39m.\u001b[39mappend(new_ann)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m annotations\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, image_processor, ann_file):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target: converting target to DETR format,\n",
    "        # resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        encoding = self.image_processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": target}\n",
    "\n",
    "\n",
    "im_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "\n",
    "path_output, path_anno = save_annotation_file_images(dataset[\"test\"])\n",
    "test_ds_coco_format = CocoDetection(path_output, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'image_id', 'width', 'height', 'objects'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwars-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
